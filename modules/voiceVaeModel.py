# -*- coding: utf-8 -*-
"""voiceVaeModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FSOeHjDL1BcYdYPbmFH7JKhyRALRYCq3
"""

# Commented out IPython magic to ensure Python compatibility.
"""
Created on Mon Apr  9 05:30:42 2020
@author: yun bin choh
"""

# %tensorflow_version 1.x

import keras
from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape
from keras.layers import BatchNormalization
from keras.models import Model
from keras.datasets import mnist
from keras.losses import binary_crossentropy
from keras import backend as K
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
import seaborn as sns


import voiceTreatment

def voiceVAE(filename_source, filename_target, FFT_SIZE=1024, batch_size = 128, no_epochs = 50, validation_split = 0.2, verbosity = 1, latent_dim_voice = 128, num_channels = 1):
'''
INPUTS : 
        - filename_source: string, example: '/content/drive/My Drive/dataset/vcc2016/wav/Training Set/SF1/*.wav'
        - filename_target: string, example: '/content/drive/My Drive/dataset/vcc2016/wav/Training Set/TM1/*.wav'
        - FFT_SIZE: int, the fast fourier transform size to determine the number of spectral lines
        - batch_size: int, mini-batch size
        - no_epochs: int, number of epochs for model training
        - validation_split: float, the ratio to split the data set into training/validation sets
        - verbosity: int, 0,1 or 2. 0 = silent, 1 = progress bar, 2 = one line per epoch
        - latent_dim_voice: int, dimension of latent space
        - num_channels: int, to reshape the data into (_,_,_,_) so as to fit into keras model
OUTPUTS:
        - encoder_voice: keras.engine.training.Model
        - decoder_voice: keras.engine.training.Model
        - vae_voice: keras.engine.training.Model
        - hist.history['loss']: list, loss over no_epochs
        - hist.history['val_loss']: list, validation loss over no_epochs
'''
      features = []
      files = tf.gfile.Glob(filename_source)
      for elem in files:
        save = voiceTreatment.analysis(elem, fft_size=FFT_SIZE, dtype=np.float32)
        for i in range(len(save)//100):
          features.append([save[i][0:513], 'SF1'])

      ## Adding in additional data for TM1
      files2 = tf.gfile.Glob(filename_target)
      for elem in files2:
        save = voiceTreatment.analysis(elem, fft_size=FFT_SIZE, dtype=np.float32)
        for i in range(len(save)//100):
          features.append([save[i][0:513], 'TM1'])

      print("From 162 utterances in SF1 & TM1 respectively, we have:",len(features),"batches")
      print("In each batch, we have:",len(features[0][0]),"spectral lines")

      # Note that different speakers have different number of batches
      count=0
      for i in range(len(features)):
        if features[i][1] == 'SF1':
          count += 1
      print('Note that different speakers have different number of batches (Due to varying length of sound files):')
      print("From 162 utterances in SF1, we have:",count,"batches")
      print("From 162 utterances in TM1, we have:",len(features)-count,"batches")
      
      # Structuring of Data
      featuresDF = pd.DataFrame(features, columns=['feature','class_label'])

      y = np.array(featuresDF.class_label.tolist())
      X = np.array(featuresDF.feature.tolist())
      from sklearn.model_selection import train_test_split 
      x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 127)

      x_train = x_train.reshape(len(x_train),1,513)
      x_test = x_test.reshape(len(x_test),1,513)

      label_encoder = LabelEncoder()
      y_train = label_encoder.fit_transform(y_train)
      y_test = label_encoder.fit_transform(y_test)

      img_width_voice, img_height_voice = x_train.shape[1], x_train.shape[2]

      # Reshape data
      input_train_voice = x_train.reshape(x_train.shape[0], img_height_voice, img_width_voice, num_channels)
      input_test_voice = x_test.reshape(x_test.shape[0], img_height_voice, img_width_voice, num_channels)
      input_shape_voice = (img_height_voice, img_width_voice, num_channels)

      # Parse numbers as floats
      input_train_voice = input_train_voice.astype('float32')
      input_test_voice = input_test_voice.astype('float32')

      # Normalize data
      input_train_voice = input_train_voice / 8
      input_test_voice = input_test_voice / 8

      # Encoder parameters/architecture
      i_voice       = Input(shape=input_shape_voice, name='encoder_input_voice')
      cx_voice      = Conv2D(filters=8, kernel_size=[7,1], strides=[3,1], padding='same', activation='relu')(i_voice)
      cx_voice      = BatchNormalization()(cx_voice)
      cx_voice     = Conv2D(filters=16, kernel_size=[7,1], strides=[3,1], padding='same', activation='relu')(cx_voice)
      cx_voice      = BatchNormalization()(cx_voice)
      x_voice      = Flatten()(cx_voice)
      x_voice       = Dense(512, activation='relu')(x_voice)
      x_voice       = BatchNormalization()(x_voice)
      mu_voice      = Dense(latent_dim_voice, name='latent_mu_voice')(x_voice)
      sigma_voice   = Dense(latent_dim_voice, name='latent_sigma_voice')(x_voice)
      conv_shape_voice = K.int_shape(cx_voice)

      # Reparametrization trick
      def sample_z_voice(args):
        mu, sigma = args
        batch     = K.shape(mu)[0]
        dim       = K.int_shape(mu)[1]
        eps       = K.random_normal(shape=(batch, dim))
        return mu + K.exp(sigma / 2) * eps

      z_voice = Lambda(sample_z_voice, output_shape=(latent_dim_voice, ), name='z')([mu_voice, sigma_voice])

      # Activation of encoder
      encoder_voice = Model(i_voice, [mu_voice, sigma_voice, z_voice], name='encoder_voice')
      #encoder_voice.summary()

      # Decoder parameters/architecture
      d_i_voice   = Input(shape=(latent_dim_voice, ), name='decoder_input_voice')
      x_voice     = Dense(conv_shape_voice[1] * conv_shape_voice[2] * conv_shape_voice[3], activation='relu')(d_i_voice)
      x_voice     = BatchNormalization()(x_voice)
      x_voice    = Reshape((conv_shape_voice[1], conv_shape_voice[2], conv_shape_voice[3]))(x_voice)
      cx_voice    = Conv2DTranspose(filters=16, kernel_size=[7,1], strides=[3,1], padding='same', activation='relu')(x_voice)
      cx_voice    = BatchNormalization()(cx_voice)
      cx_voice    = Conv2DTranspose(filters=8, kernel_size=[7,1], strides=[3,1], padding='same',  activation='relu')(cx_voice)
      cx_voice    = BatchNormalization()(cx_voice)
      o_voice     = Conv2DTranspose(filters=num_channels, kernel_size=[7,1], activation='sigmoid', padding='same', name='decoder_output_voice')(cx_voice)

      # Activation of decoder
      decoder_voice = Model(d_i_voice, o_voice, name='decoder_voice')
      #decoder_voice.summary()

    
      # Activation of VAE model
      vae_outputs_voice = decoder_voice(encoder_voice(i_voice)[2])
      vae_voice         = Model(i_voice, vae_outputs_voice, name='vae_voice')
      #vae_voice.summary()

      # Defining the loss function
      def kl_reconstruction_loss_voice(true, pred):
        # Reconstruction loss
        reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_width_voice * img_height_voice
        # KL divergence loss
        kl_loss = 1 + sigma_voice - K.square(mu_voice) - K.exp(sigma_voice)
        kl_loss = K.sum(kl_loss, axis=-1)
        kl_loss *= -0.5
        # Total loss = 50% rec + 50% KL divergence loss
        return K.mean(reconstruction_loss + kl_loss)

      # Compile VAE
      vae_voice.compile(optimizer='adam', loss=kl_reconstruction_loss_voice)

      # Train autoencoder
      hist = vae_voice.fit(input_train_voice, input_train_voice, epochs = no_epochs, batch_size = batch_size, validation_split = validation_split)

      golden_size = lambda width: (width, 2. * width / (1 + np.sqrt(5)))
      #NELBO
      fig, ax = plt.subplots(figsize=golden_size(6))

      hist_df = pd.DataFrame(hist.history)
      hist_df.plot(ax=ax)

      ax.set_ylabel('NELBO')
      ax.set_xlabel('# epochs')

      ax.set_ylim(.99*hist_df[1:].values.min(), 
                  1.1*hist_df[1:].values.max())


      plt.show()

      # Latent space visualization for first 2 dimensions
      def viz_latent_space_voice(encoder_voice, data):
        input_data, target_data = data
        mu, _, _ = encoder_voice.predict(input_data)
        plt.figure(figsize=(8, 10))
        plt.scatter(mu[:, 0], mu[:, 1], c=target_data)
        plt.xlabel('z - dim 1')
        plt.ylabel('z - dim 2')
        plt.colorbar()
        plt.show()

      data_v = (input_test_voice, y_test)
      viz_latent_space_voice(encoder_voice, data_v)

      # Latent space visualization for first n-dimensions where 2 <n <=128
      store = []
      for i in range(len(encoder_voice.predict(input_test_voice)[0])):
        store.append(encoder_voice.predict(input_test_voice)[0][i])

      storeFrame = pd.DataFrame(store)
      #storeFrame = storeFrame.transpose()

      pp = sns.pairplot(storeFrame.loc[:, 0:9], size=1.8, aspect=1.8,
                        plot_kws=dict(edgecolor="k", linewidth=0.5),
                        diag_kind="kde", diag_kws=dict(shade=True))

      fig = pp.fig 
      fig.subplots_adjust(top=0.93, wspace=0.3)
      t = fig.suptitle('Voice Latent Space Representation Pairwise Plots', fontsize=14)

      return encoder_voice, decoder_voice, vae_voice, hist.history['loss'], hist.history['val_loss']
